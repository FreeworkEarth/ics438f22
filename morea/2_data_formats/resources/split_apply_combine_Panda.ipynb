{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449f7aab",
   "metadata": {},
   "source": [
    "### Popular Big Data Fromats\n",
    "\n",
    "* Data format is an important aspect of working with big data\n",
    "\n",
    "* The recurring topic is \"There ain't such a thing as free lunch\"\n",
    "\n",
    "```\"There ain't no such thing as a free lunch\" (TANSTAAFL), also known as \"there is no such thing as a free lunch\" (TINSTAAFL), is an expression that describes the cost of decision-making and consumption. The expression conveys the idea that things appearing free always have some cost paid by somebody, or that nothing in life is truly free. ``` **https://www.investopedia.com/terms/t/tanstaafl.asp**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce05a6",
   "metadata": {},
   "source": [
    "### Popular Big Data Formats\n",
    "\n",
    "* Some of the issues that arise when working with data formats are:\n",
    "\n",
    "1. Compression\n",
    "  * Not all file formats are equally compressible with the same algorithm.\n",
    "2. Splittability\n",
    "  * How splittable is a file format?\n",
    "  * as we saw in split apply combine, being able to split a file and run across multiple machines can be critical in some instances\n",
    "3. Columnar and row-wise data formats\n",
    "  * Not all the columns (variables) in a dataset are equally valuable analytically.\n",
    "  * Being able to compute column-based stats can force the adoption of a format that makes it easier to extract column data\n",
    "4. Data Types and Schema Evolution\n",
    "  * Do we need to enforce data types?\n",
    "  * Will my data format change over time?\n",
    "  * With petabytes of data, it is not reasonable to think that we can just regenerate the files every time there is a change to the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed420e46",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition\n",
    "\n",
    "* In big data, the right storage format is paramount for achieving performance, saving space, and making certain operations possible.\n",
    "\n",
    "* Can save time, minimize complexity and decrease cost.\n",
    "\n",
    "* We're accustomed to row-based formats\n",
    "\n",
    "  * MS Excel file-like where each row is a table entry\n",
    "\n",
    "| Transaction Date     | Nb Items     | Total       |\n",
    "|------------------    |----------    |---------    |\n",
    "| 01/01/2001           | 4            | 1852.14     |\n",
    "| 01/01/2001           | 3            | 968.00      |\n",
    "| `...`             | `...`     | `...`     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8017d4a",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition - Cont'd\n",
    "    \n",
    "* This format may be inappropriate for certain types of data or operations\n",
    "\n",
    "* Imagine that the sales info above contains a very large number of transactions with hundreds of thousands of transactions each day\n",
    "    * The same transaction dates will be unnecessarily duplicated hundreds of thousands of times.\n",
    "    * Perhaps a dictionary-like format where the key is the date would help save on storage\n",
    " \n",
    "```python\n",
    "{\"01/01/2001\": ((4, 1852.14), (3,  968.00), ...), \"01/02/2001\":(...), ... }\n",
    "```\n",
    "* This will also be more efficient for computing operations on days\n",
    "   * E.g. count number of transactions or total sales per day\n",
    "   * What about computing the running sales total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4e83b",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition - Cont'd\n",
    "\n",
    "* If the objective were to calculate the total sales, we would need to read millions of lines to compute a single value.\n",
    "* Perhaps we can store the data as row data. Reading a single line is sufficient to compute the average.\n",
    "\n",
    "|              |      |  |   |\n",
    "| :---              |    :----:  | :--------: |:---:|\n",
    "| **Totals**             | 1852.14    | 968.00     | `...` |\n",
    "| **Transaction Dates** | 01/01/2001 | 01/01/2001 | `...` |\n",
    "| **Nb Items**               | 4             | 3          | `...` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cc7fe",
   "metadata": {},
   "source": [
    "### File Formats Decisions\n",
    "\n",
    "* There are four considerations when selecting file formats:\n",
    "    1. Row vs Column\n",
    "      * What kind of analytics are important?\n",
    "    2. Schema Management\n",
    "      * Will my data schemas evolve?\n",
    "    3. Spilitability\n",
    "      * Can I store data across multiple files and potentially servers?\n",
    "    4. Compression\n",
    "      * How compressible is the format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfd1dc",
   "metadata": {},
   "source": [
    "### 1. Row- and Column-Based Formats\n",
    "\n",
    "* An important consideration when selecting a big data format\n",
    "\n",
    "![](https://www.dropbox.com/s/an5fg7xl2uvnfb8/row_col_format.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2bb0e",
   "metadata": {},
   "source": [
    "### 1. Row- and Column-Based Formats\n",
    "\n",
    "* Row-based: Ideal when using all the data\n",
    "  * Example, building a machine learning model that requires all the features and all the instances\n",
    "    * Avoid reading all the dataset in RAM by loading chunks at a time\n",
    "    * Required frequent conditional access to multiple colums \n",
    "\n",
    "* Column-based storage: useful when performing operations on a subset of columns\n",
    "  * Computing total sales, or computing a total aggregated by aggregated by date, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b5853",
   "metadata": {},
   "source": [
    "### Row-Based Formats\n",
    "\n",
    "* Used in most mainstream applications, from web log files to highly-structured database systems like MySQL and Oracle.\n",
    "\n",
    "* Processing all the data would require reading all inputs line by line\n",
    "\n",
    "* This is commonly used for Online Transactional Processing (OLTP).\n",
    "\n",
    "  * OLTP systems usually process CRUD queries (Create, Read, Update and Delete) at a record level.\n",
    "\n",
    "  * The main emphasis for OLTP systems is maintaining data integrity in multi-access environments\n",
    "\n",
    "* Effectiveness measured by the number of transactions per second\n",
    "\n",
    "   * More on this when we discuss big data platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e001a",
   "metadata": {},
   "source": [
    "### Column-Based Formats\n",
    "\n",
    "\n",
    "* The data is grouped by columns\n",
    "* Easy to focus computation on specific columns of data\n",
    "  * E.g.: compute the mean or standard deviation of a column, search for the largest\n",
    "    * What do you think this last operation is less computationally intensive on a column-based format?  \n",
    "\n",
    "* Ideal for compression\n",
    "  * Compression codecs (e.g., GZIP, pkzip, etc..) have a higher compression ratio when compressing sequences of similar data types. \n",
    "\n",
    "  \n",
    "\n",
    "```python\n",
    "[[1,2,3,..], [\"John\", \"Janet\", \"Michael\", ...], ...]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* If much more efficient than compressing:\n",
    "\n",
    "```python\n",
    "[[1, \"John\", \"Doe\", \"125,000\"], [2, \"Janet\", \"Smith\", \"195,129\"], ...]\n",
    "```\n",
    "\n",
    "* Typically, the slowest components in large distributed systems are the disk and network\n",
    "    * Using compression reduces read IO and transfers, thus speeding up the analysis.\n",
    "\n",
    "\n",
    "* This way of processing data is usually called OLAP (Online Analytical Processing)   \n",
    "\n",
    " * OLAP is an approach designed to quickly answer analytics queries involving multiple features (variables), typically across database systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf15a2b",
   "metadata": {},
   "source": [
    "### Compression of Row vs. Columnar Data\n",
    "\n",
    "  * Let's perform a quick exrperiment \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b956e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices([1,2,3,4], k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a583d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'T', 'G', 'C', 'C', 'T']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices(\"ACGT\", k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e5d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "print(string.printable)\n",
    "print(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b23c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8414\n",
      "5081\n"
     ]
    }
   ],
   "source": [
    "import zlib \n",
    "import string\n",
    "\n",
    "# let's randomly generate two string of 1000, an ASCII and an INT\n",
    "\n",
    "random_ASCII = random.choices(string.printable, k=10_000)\n",
    "random_numbers = random.choices(string.digits, k=10_000)\n",
    "print(len(zlib.compress( str.encode(\"\".join(random_ASCII)))))\n",
    "print(len(zlib.compress( str.encode(\"\".join(random_numbers)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0f4f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6547445239154617"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "ratios = []\n",
    "for i in range(10):\n",
    "    random_ASCII = random.choices(string.printable, k=10_000)\n",
    "    random_numbers = random.choices(string.digits, k=10_000)\n",
    "    len_ascii = len(zlib.compress( str.encode(\"\".join(random_ASCII)))) \n",
    "    len_numbers = len(zlib.compress( str.encode(\"\".join(random_numbers))))\n",
    "    ratios.append(len_ascii/len_numbers)\n",
    "    \n",
    "numpy.mean(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad2327eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f565ea35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3462144408226877"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios = []\n",
    "for i in range(10):\n",
    "    random_ASCII = random.choices(string.printable, k=10_000)\n",
    "    random_uppercase = random.choices(string.ascii_uppercase, k=10_000)\n",
    "    len_ascii = len(zlib.compress( str.encode(\"\".join(random_ASCII)))) \n",
    "    len_uppercase = len(zlib.compress( str.encode(\"\".join(random_uppercase))))\n",
    "    ratios.append(len_ascii/len_uppercase)    \n",
    "numpy.mean(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a083d77",
   "metadata": {},
   "source": [
    "### Examples of OLAP versus OLTP in Amazon\n",
    "\n",
    "![](https://www.dropbox.com/s/cxhwtc5s582tnp2/amazon_olap_oltp.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279abc9",
   "metadata": {},
   "source": [
    "### Column-based formats: Advantages and Disadvantages\n",
    "\n",
    "<u>Advantages</u>:\n",
    "* Columnar storage of data can sometimes yield 100x-1000x performance improvements, particularly for wide datasets\n",
    "\n",
    "\n",
    "<u>Disadvantages</u>:\n",
    "  *  Not efficient with CRUD operations\n",
    "  * Difficult to access all features of a single instance\n",
    "    * Need to parse all columns to position $i$\n",
    "  * Hard to read by a human\n",
    "  * Can be more CPU intensive to write for very large data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a4a3e",
   "metadata": {},
   "source": [
    "### 2- Datatype and Schema Enforcement and Evolution\n",
    "\n",
    "* \"Schema\" in a database context, means the structure and organization of the data  \n",
    "    * Structure: datatypes, missing values, primary keys, etc, indices, etc.\n",
    "    * Organization: relationships across tables.\n",
    "\n",
    "* Here, we mainly refer to the data type\n",
    "* In text format, (e.g.: table with values separated by space), datatype cannot be declared or enforced\n",
    "\n",
    "* Declaring the type of a value provides some advantages.\n",
    "  * Storage requirements: String categories will require more storage than boolean (2 bytes)\n",
    "  * Data validity: Verifies the dataset is valid and prevents entry errors (e.g., age = Johnn)\n",
    "  * Compression: there are good strategies for compressing different data types "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c5229",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution - Cont'd\n",
    "\n",
    "\n",
    "* In the event that there is no guarantee that data won't change in the future, you may need to consider schema evolution.\n",
    "\n",
    "\n",
    "* When evaluating schema evolution, there are a few key questions to ask of any data format:\n",
    "  * How easy is it to update a schema (such as adding a field, removing or renaming a field)?\n",
    "  * How will different versions of the schema impact applications?\n",
    "  * How fast can the schema be processed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de7933",
   "metadata": {},
   "source": [
    "### 3- Splitability\n",
    "\n",
    "* Big data such as monthly logs, yearly transactions, daily airplane sensors recordings, can often comprise many millions of records.\n",
    "\n",
    "* Often useful to split the data across multiple machines and execute each computation separately\n",
    "\n",
    "* Some file formats are more amenable to splitting than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebb546",
   "metadata": {},
   "source": [
    "### 3- Splitability - Row-based\n",
    "\n",
    "Row-based formats can be split along row boundaries\n",
    "\n",
    "```\n",
    "# file 1 with n lines\n",
    "01/01/2001           4            1852.14\n",
    "01/01/2001           3            968.00\n",
    "...\n",
    "```\n",
    "\n",
    "* Splitting can be done\n",
    "  * Randomly plitting `file 1` with `n` observations across `m` total machines is easy.\n",
    "\n",
    "    * Each machine gets `ceiling(n/m)` unique lines, last machine gets remaining lines\n",
    "\n",
    " * Splitting based on one or more fields: \n",
    "    * Partitioning a rown-based file over particular column values can be difficult if data is stored in a random order.\n",
    "    * May require sorting the data first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f3d5a",
   "metadata": {},
   "source": [
    "### 3- Splitability row-based, nested \n",
    "\n",
    "* Larg column-based data can be more difficult to split\n",
    "\n",
    "``` \n",
    "file 2\n",
    "{\"01/01/20014\": [(4, 1852.14), (3, 968.00)], ....}\n",
    "```\n",
    "\n",
    "* You cannot easily split this file this file format without parsing the file first.\n",
    "  * Need to read the compelte file to split it into chunks.\n",
    "    * Data may need to ne loaded in RAM first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aad4d4",
   "metadata": {},
   "source": [
    "### 3- Splitability: Column-based, nested\n",
    "\n",
    "\n",
    "* A column-based format can be split if the comutation is column-specific.\n",
    "\n",
    "```\n",
    "# file 3\n",
    "date: 01/01/2001, 01/01/2001\n",
    "nb_items: 4, 3\n",
    "totals: 1852.14, 968.00\n",
    "```\n",
    "\n",
    "Splitting can only e done column-wise:\n",
    "* In the example above, each machine is concerned with a computation on a specific variable. For example:\n",
    "  * Machine 1 takes `date` data and computes the number of sales per month\n",
    "  * Machine 2 takes the `nb_items` data and computes the total number of sales\n",
    "  * Machine 3 takes the `totals` data and computes the total sales values\n",
    " \n",
    "* Machines don't have any knowledge of variables that are not given.\n",
    "  * E.g., if machine three is not given date info and cannot compute, for example, the monthly or weekly sales average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a8c1",
   "metadata": {},
   "source": [
    "### 4- Compression\n",
    "\n",
    "\n",
    "* When working on a distributed system, data transfer can be a serious bottleneck\n",
    "* Compression can substantially improve runtime and storage requirements\n",
    "\n",
    "* We illustrated \"naively\" that columnar data can achieve better compression rates than row-based data\n",
    "  * Simple way to think about it: column will have a lot more duplicate values:\n",
    "      * Ex. Age Column: 21, 22, 21, 24, 25, 21, 22, 21, 19, 21, 21, 22, ....\n",
    "      \n",
    "* Note that complex compression algorithms on very large files can save on space but substantially increase compute time.\n",
    "    * Uncompression/re-compression needs to occur every time you need to access the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6279c",
   "metadata": {},
   "source": [
    "### Standardization and File Formats\n",
    "\n",
    "* One can always choose their own format for the file\n",
    "  * Many companies may choose to do so internally for many reasons.\n",
    "  * E.g.:\n",
    "\n",
    "```\n",
    "FIRST_NAME_1\\sLAST_NAME_1\\tFIRST_NAME_2\\sLAST_NAME_2\\tFIRST_NAME_3\\sLAST_NAME_3...\n",
    "JOBTITLE_1\\sSALARY_1\\tJOBTITLE_2\\sSALARY_2\\tJOBTITLE_3\\sSALARY_3\n",
    "```\n",
    "\n",
    "* However, there are many benefits to using a standard file format. E.g.:\n",
    "  * Clarity and productivity: eliminating the need for guesswork or extra searching for answer. Plus there is no need to maintain internal documentation, which makes it easier to get answers online when issues arise.\n",
    "\n",
    "  * Quality: standard formats are designed by large teams and used extensively, which provides opportunities to optimize them\n",
    "\n",
    "  * Interoperability: your data is no longer locked to your company (or compartmentalized) and can be used across platforms.\n",
    "\n",
    "* Some of the most used formats are CSV, JSON, Parquet, AVRO, HDF5\n",
    "  * All very well supported in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0ff77",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* Files in the CSV (Comma-separated values) format are usually used to exchange tabular data\n",
    "  * Plain-text file (readable characters)\n",
    " \n",
    "* CSV is a row-based file format: each row of the file is a separate data instance\n",
    "  * May or may not contain a header\n",
    "* Structure is conveyed through explicit commas\n",
    "  * Text commas are encapsulated in double quotes\n",
    "\n",
    "```\n",
    "Title,Author,Genre,Height,Publisher\n",
    "\"Computer Vision, A Modern Approach\",\"Forsyth, David\",data_science,255,Pearson\n",
    "Data Mining Handbook,\"Nisbet, Robert\",data_science,242,Apress\n",
    "Making Software,\"Oram, Andy\",computer_science,232,O'Reilly\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2028",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* CSV format is not fully standardized\n",
    "  * Other characters can be used to separate files, such as tabs (tsv) or spaces (ssv)\n",
    " \n",
    "* Data relationships across multiple CSV files are not expressed in the file format\n",
    "  * Use same column names to indicate \"foreign key\" relationship\n",
    " \n",
    "\n",
    "* Native support in Python\n",
    "```python\n",
    "import csv\n",
    "csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "# use csv ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e31127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: ['Rank', 'Year', 'Movie', 'WorldwideBox Office', 'DomesticBox Office', 'InternationalBox Office']\n",
      "Line 1: ['1', '2009', 'Avatar', '$2,845,899,541', '$760,507,625', '$2,085,391,916']\n",
      "Line 2: ['2', '2019', 'Avengers: Endgame', '$2,797,800,564', '$858,373,000', '$1,939,427,564']\n",
      "Line 3: ['3', '1997', 'Titanic', '$2,207,986,545', '$659,363,944', '$1,548,622,601']\n",
      "Line 4: ['4', '2015', 'Star Wars Ep. VII: The Force Awakens', '$2,064,615,817', '$936,662,225', '$1,127,953,592']\n",
      "Line 5: ['5', '2018', 'Avengers: Infinity War', '$2,044,540,523', '$678,815,482', '$1,365,725,041']\n",
      "Line 6: ['6', '2015', 'Jurassic World', '$1,669,979,967', '$652,306,625', '$1,017,673,342']\n",
      "Line 7: ['7', '2019', 'The Lion King', '$1,654,367,425', '$543,638,043', '$1,110,729,382']\n",
      "Line 8: ['8', '2015', 'Furious 7', '$1,516,881,526', '$353,007,020', '$1,163,874,506']\n",
      "Line 9: ['9', '2012', 'The Avengers', '$1,515,100,211', '$623,357,910', '$891,742,301']\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "with open('All_Time_Worldwide_Box_Office.csv')  as csvfile:\n",
    "    movies_file = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    i = 0 \n",
    "    for line in movies_file:\n",
    "        print(f\"Line {i}: {line}\")\n",
    "        i+=1\n",
    "        if i ==10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a5b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: {'Rank': '1', 'Year': '2009', 'Movie': 'Avatar', 'WorldwideBox Office': '$2,845,899,541', 'DomesticBox Office': '$760,507,625', 'InternationalBox Office': '$2,085,391,916'}\n",
      "Line 1: {'Rank': '2', 'Year': '2019', 'Movie': 'Avengers: Endgame', 'WorldwideBox Office': '$2,797,800,564', 'DomesticBox Office': '$858,373,000', 'InternationalBox Office': '$1,939,427,564'}\n",
      "Line 2: {'Rank': '3', 'Year': '1997', 'Movie': 'Titanic', 'WorldwideBox Office': '$2,207,986,545', 'DomesticBox Office': '$659,363,944', 'InternationalBox Office': '$1,548,622,601'}\n",
      "Line 3: {'Rank': '4', 'Year': '2015', 'Movie': 'Star Wars Ep. VII: The Force Awakens', 'WorldwideBox Office': '$2,064,615,817', 'DomesticBox Office': '$936,662,225', 'InternationalBox Office': '$1,127,953,592'}\n",
      "Line 4: {'Rank': '5', 'Year': '2018', 'Movie': 'Avengers: Infinity War', 'WorldwideBox Office': '$2,044,540,523', 'DomesticBox Office': '$678,815,482', 'InternationalBox Office': '$1,365,725,041'}\n",
      "Line 5: {'Rank': '6', 'Year': '2015', 'Movie': 'Jurassic World', 'WorldwideBox Office': '$1,669,979,967', 'DomesticBox Office': '$652,306,625', 'InternationalBox Office': '$1,017,673,342'}\n",
      "Line 6: {'Rank': '7', 'Year': '2019', 'Movie': 'The Lion King', 'WorldwideBox Office': '$1,654,367,425', 'DomesticBox Office': '$543,638,043', 'InternationalBox Office': '$1,110,729,382'}\n",
      "Line 7: {'Rank': '8', 'Year': '2015', 'Movie': 'Furious 7', 'WorldwideBox Office': '$1,516,881,526', 'DomesticBox Office': '$353,007,020', 'InternationalBox Office': '$1,163,874,506'}\n",
      "Line 8: {'Rank': '9', 'Year': '2012', 'Movie': 'The Avengers', 'WorldwideBox Office': '$1,515,100,211', 'DomesticBox Office': '$623,357,910', 'InternationalBox Office': '$891,742,301'}\n",
      "Line 9: {'Rank': '10', 'Year': '2019', 'Movie': 'Frozen II', 'WorldwideBox Office': '$1,446,925,396', 'DomesticBox Office': '$477,373,578', 'InternationalBox Office': '$969,551,818'}\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "with open('All_Time_Worldwide_Box_Office.csv')  as csvfile:\n",
    "    movies_file = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "    i = 0 \n",
    "    for line in movies_file:\n",
    "        print(f\"Line {i}: {line}\")\n",
    "        i+=1\n",
    "        if i ==10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a748ae3",
   "metadata": {},
   "source": [
    "### CSV Pros and Cons\n",
    "<u>Pros:</u>\n",
    "* Human-readable and easy to edit manually\n",
    "* Provides a simple scheme\n",
    "* Can be processed by almost all existing applications\n",
    "* Easy to implement and parse;\n",
    "* Compact (compared to, for instance JSON or MXL)\n",
    "* Column headers are written only once\n",
    "\n",
    "<u>Cons:</u>\n",
    "* No guarantees about data integrity, i.e., data won't be missing or won't be in a different type than expected.\n",
    "* Adding complex structures to a data structure is not possible\n",
    "  * May need to reference other files to implement nesting\n",
    "* There is no standard way to present binary data\n",
    "* Lack of a universal standard can cause "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb872ea",
   "metadata": {},
   "source": [
    "### JSON File Format\n",
    "\n",
    "* JSON (JavaScript Object Notation)\n",
    "\n",
    "* Open standard file format that uses human-readable text\n",
    "  * FIle typically stored using `.json` extension\n",
    "  \n",
    "* Became popular as a space-saving alternative to Extensible Markup Language (XML)\n",
    "\n",
    "* Inspired by JavaScript objects but is a language-independent data format\n",
    "\n",
    "* Very similar to the combination of Python's lists and dicts\n",
    "\n",
    "* Also supported natively in Python\n",
    "  ```python\n",
    "  import json\n",
    "json.load(...)\n",
    "  ```\n",
    "* The defacto language of the web\n",
    "  * Supported in all modern languages and particularly web languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11c99f",
   "metadata": {},
   "source": [
    "### JSON File Structure\n",
    "\n",
    "* JSON supports the following types.\n",
    "\n",
    "* Scalar values\n",
    "    * `Numbers`: e.g. 3\n",
    "    * `String`: Sequence of Unicode characters surrounded by double quotation marks.\n",
    "    * `Boolean`: `true` or `false`.\n",
    "\n",
    "* Collections:\n",
    "    * `Array`: A list of values surrounded by square brackets `[]`\n",
    "    * `Dictionaries`: key\" value pairs separated by a comma(,) and enclosed in `{}`\n",
    "      * Keys are strings and values can be any valid scalar or collection\n",
    "\n",
    "* See the following for more details: https://docs.fileformat.com/web/json/\n",
    "* See the following very good (useful) validator for validating JSON files or records: https://jsonformatter.curiousconcept.com/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16187bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'Salary': 120000,\n",
       "  'volunteer': False},\n",
       " {'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'salary': None,\n",
       "  'volunteer': True}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = [ \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"Salary\": 120_000, \"volunteer\": False}, \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"salary\": None, \"volunteer\": True}\n",
    "]\n",
    "my_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c69d70",
   "metadata": {},
   "source": [
    "```python\n",
    "json.load\n",
    "json.loads\n",
    "json.dump\n",
    "json.dumps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cd4c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"First Name\": \"John\", \"Occupation\": \"Student\", \"Salary\": 120000, \"volunteer\": false}, {\"First Name\": \"John\", \"Occupation\": \"Student\", \"salary\": null, \"volunteer\": true}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_representation = json.dumps(my_data)\n",
    "print(json_representation)\n",
    "# Note the changes between the Python dict and the JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42c6ca",
   "metadata": {},
   "source": [
    "### Working with the Python `json` library\n",
    "\n",
    "\n",
    "* `All_Time_Worldwide_Box_Office_partial.json`  structure\n",
    "```json\n",
    "[\n",
    " {\n",
    "  \"Rank\": \"1\",\n",
    "  \"Year\": \"2009\",\n",
    "  \"Movie\": \"Avatar\",\n",
    "  \"WorldwideBox Office\": \"$2,845,899,541\",\n",
    "  \"DomesticBox Office\": \"$760,507,625\",\n",
    "  \"InternationalBox Office\": \"$2,085,391,916\"\n",
    " },\n",
    " {\n",
    "  \"Rank\": \"2\",\n",
    "  \"Year\": \"2019\",\n",
    "  \"Movie\": \"Avengers: Endgame\",\n",
    "  \"WorldwideBox Office\": \"$2,797,800,564\",\n",
    "  \"DomesticBox Office\": \"$858,373,000\",\n",
    "  \"InternationalBox Office\": \"$1,939,427,564\"\n",
    " },\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b546b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rank': '1',\n",
       "  'Year': '2009',\n",
       "  'Movie': 'Avatar',\n",
       "  'WorldwideBox Office': '$2,845,899,541',\n",
       "  'DomesticBox Office': '$760,507,625',\n",
       "  'InternationalBox Office': '$2,085,391,916'},\n",
       " {'Rank': '2',\n",
       "  'Year': '2019',\n",
       "  'Movie': 'Avengers: Endgame',\n",
       "  'WorldwideBox Office': '$2,797,800,564',\n",
       "  'DomesticBox Office': '$858,373,000',\n",
       "  'InternationalBox Office': '$1,939,427,564'},\n",
       " {'Rank': '3',\n",
       "  'Year': '1997',\n",
       "  'Movie': 'Titanic',\n",
       "  'WorldwideBox Office': '$2,207,986,545',\n",
       "  'DomesticBox Office': '$659,363,944',\n",
       "  'InternationalBox Office': '$1,548,622,601'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_file = open('data/All_Time_Worldwide_Box_Office_partial.json') \n",
    "movies_data = json.load(json_file)\n",
    "movies_data[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7341ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d5c94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eac5ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie Avatar, grossed $2,845,899,541 in 2009\n",
      "The movie Avengers: Endgame, grossed $2,797,800,564 in 2019\n",
      "The movie Titanic, grossed $2,207,986,545 in 1997\n",
      "The movie Star Wars Ep. VII: The Force Awakens, grossed $2,064,615,817 in 2015\n",
      "The movie Avengers: Infinity War, grossed $2,044,540,523 in 2018\n",
      "The movie Jurassic World, grossed $1,669,979,967 in 2015\n",
      "The movie The Lion King, grossed $1,654,367,425 in 2019\n",
      "The movie Furious 7, grossed $1,516,881,526 in 2015\n",
      "The movie The Avengers, grossed $1,515,100,211 in 2012\n",
      "The movie Frozen II, grossed $1,446,925,396 in 2019\n"
     ]
    }
   ],
   "source": [
    "for record in movies_data:\n",
    "    print(f\"The movie {record['Movie']}, grossed {record['WorldwideBox Office']} in {record['Year']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2091ef8",
   "metadata": {},
   "source": [
    "### JSON Pros and Cons\n",
    "\n",
    "* Pros:\n",
    "    * Very well supported in modern languages, technologies and infrastructures\n",
    "    * Can be used as the basis for more performance-optimized formats Parquet or Avro (discussed next)\n",
    "    * Supports hierarchical structures abstracting the need for complex relationships\n",
    "    * The *defacto* standard in NoSQL databases\n",
    "* Cons:\n",
    "    * Much smaller footprint than XML but still fairly large due to repeated field names\n",
    "    * Not easy to index\n",
    "    * Some tentatives to add a schema but not commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d7204",
   "metadata": {},
   "source": [
    "### AVRO File Format\n",
    "\n",
    "* AVRO is an advanced form of the JSON format\n",
    "    * Leverages some of the advantages of JSON while mitigating some of its disadvantages\n",
    "* Uses a file definition (a schema itself written in JSON) and stores data without the repeated field names.\n",
    "  * Said to be self-descriptive because you can include the schema and documentation in the header of the file containing the data\n",
    "\n",
    "* Released by the Hadoop working group in 2009 to use with Hadoop Systems\n",
    "* It is a row-based format that has a high degree of splitting\n",
    "* Provides mechanism to manage schema evolution\n",
    "* support for most modern languages, including Python via the `avro` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1779595",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "* Pros:\n",
    "    * Binary data minimizes file size and maximizes efficiency\n",
    "    * A reliable support for schema evolution\n",
    "      * Supports new, missing, or changed fields.\n",
    "      * This allows old software to read new data, and new software to read old data\n",
    "      * It is a critical feature if your data can change.\n",
    "* Cons:\n",
    "    * Data is not human readable\n",
    "    * all the cons of a row-based format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0ce0c",
   "metadata": {},
   "source": [
    "### PARQUET Format\n",
    "\n",
    "\n",
    "* Parquet was developed by Twitter and Cloudera as a columnar data store\n",
    "* Parquet is especially useful with wide datasets (datasets with many columns)\n",
    "\n",
    "* Optimized for reading and is therefore ideal for read-intensive workloads\n",
    "* Parquet was also designed to support columnar partitions\n",
    "    * Splitting the data based on value similarity, whcih results in a folder hierarchy\n",
    "      * E.g.: split on the similar values of the MONTH  or department\n",
    "    * Splits can be nested by splitting on a second attribute.\n",
    "      * Will result in a nested folder hierarchy\n",
    "     \n",
    "```      \n",
    "    MONTH=JANUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "        \n",
    "    MONTH=FEBRUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "\n",
    "    ...\n",
    "      \n",
    "```  \n",
    "https://blog.datasyndrome.com/python-and-parquet-performance-e71da65269ce\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e882",
   "metadata": {},
   "source": [
    "### PARQUET PROS and CONS\n",
    "\n",
    "* Pros: \n",
    " * Highly compressible since data is stored column-wise (compression rates up to 75%)\n",
    "   * Can use different compression algorithms with different datatypes\n",
    " * Seamless splittability across columns.\n",
    " * Optimized for reading data and ideal for read-intensive tasks\n",
    "   * Can use parallelization to read different columns.\n",
    "\n",
    "\n",
    "\n",
    "* Cons:\n",
    " * Very slow at writing data and not good with write-intensive applications\n",
    " * Does not support updates on the data as Parquet files are immutable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Mapping for working with Large Files\n",
    "\n",
    "* Memory Mapping is a power concepts to map a portion or the complete file on disk  to a some virtual memory.\n",
    "\n",
    "* Application can access segments of a file without having to first read the complete file in memory.\n",
    "\n",
    "* Ideal for approach that support streaming-like processes instead of indexing.\n",
    "  * Think for instance of map-like operations.\n",
    "\n",
    "* Allows you to use memory access rather than read and write. \n",
    "\n",
    "* Kernel schedule reads and writes of physical pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e04878",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better Memory Managment\n",
    "\n",
    "* Read only data you need\n",
    "  * Cols or subfiles\n",
    "* Specify the column type to avoid loading the data into memory\n",
    "  * even if the data fits, the read may take too much memory\n",
    "* use categories, instead of strings when possible \n",
    "  * Use appropriate data types, int8 will take less memroy than an int 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6188eb",
   "metadata": {},
   "source": [
    "### Memory Mapping using `mmap` \n",
    "\n",
    "* Locality in computing leads to huge performance gains.\n",
    "  * Having data on registers or in RAM is comptuationally efficient\n",
    "  * Swapping degrades performance\n",
    "\n",
    "* Mmap is `C` function to map or unmap files or devices into memory\n",
    "\n",
    "```\n",
    "It is a method of memory-mapped file I/O. It implements demand paging because file contents are not read from disk directly and initially do not use physical RAM at all. The actual reads from disk are performed in a \"lazy\" manner, after a specific location is accessed.```\n",
    "* https://en.wikipedia.org/wiki/Memory-mapped_file\n",
    "\n",
    "* Reading, seeking and writing are compared to in-memoy pointer operations\n",
    "\n",
    "\n",
    "* Memory is allocated to a process\n",
    "    * Processes may need to communicate via shared memroy and mmap makes that to share files files\n",
    "    * Simplified task parallelization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844eca3",
   "metadata": {},
   "source": [
    "\n",
    "### Advantages of mmap \n",
    "\n",
    "* No intermediate presentation of the the data\n",
    "  * less overhead for the data.\n",
    "* Data sharing across processes.\n",
    "* Modifications are immediate.\n",
    "  * modified a file handle does not modify the file. It still needs to be written to file\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d69af33",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/san_francisco_2015.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/san_francisco_2015.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39minfo(memory_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/s3-next/lib/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/san_francisco_2015.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/san_francisco_2015.csv\")\n",
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/san_francisco_2015.csv\")\n",
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_handle = open(\"data/san_francisco_2015.csv\", \"r+b\") \n",
    "mmap_file = mmap.mmap(file_handle.fileno(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b60c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = mmap_file.readline()\n",
    "print(f\"Line 0:\\t{line}\")\n",
    "line = mmap_file.readline()\n",
    "print(f\"Line 1:\\t{line}\")\n",
    "line = mmap_file.readline()\n",
    "print(f\"Line 1:\\t{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file.seek(0)\n",
    "line = mmap_file.readline()\n",
    "print(f\"Line:\\t{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file.seek(0)\n",
    "mmap_file.find(b'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file.seek(282)\n",
    "mmap_file.find(b'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f258b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ff00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = open(\"data/san_francisco_2015.csv\", 'r').read()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf840f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mmap_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00441bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(mmap_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6784ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(mmap_file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_file[0:9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost equivalent to \n",
    "[x.to_bytes(1, 'big') for x in mmap_file[0:9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to \n",
    "b''.join([x.to_bytes(1, 'big') for x in mmap_file[0:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# almsot equivalent to \n",
    "\"\".join([chr(x) for x in mmap_file[0:9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90b6e1",
   "metadata": {},
   "source": [
    "### The Dataset Library\n",
    "* Combines `Arrow`'s columnar format\n",
    "* Using mmap, we can share data across processes\n",
    "  * allows zero-copy reads which removes virtually all serialization overhead.\n",
    "* Arrow is language-agnostic so it supports different programming languages.\n",
    "* Arrow is column-oriented so it is faster at querying and processing slices or columns of data.\n",
    "* Arrow allows for copy-free hand-offs to standard machine learning tools such as NumPy, Pandas, PyTorch, and TensorFlow.\n",
    "* Arrow supports many, possibly nested, column types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff665d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f50b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "#my_data = load_dataset(\"san_francisco_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.dropbox.com/s/an5fg7xl2uvnfb8/row_col_format.png?dl=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
